{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expressed-hacker",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree, ensemble\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import time\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "secret-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_path = '../../Data/filtered_status.csv'\n",
    "stations_path = '../../Data/station.csv'\n",
    "\n",
    "status_df = pd.read_csv(status_path, parse_dates=['time'])\n",
    "stations_df = pd.read_csv(stations_path)\n",
    "\n",
    "San_Fancisco_stations = stations_df[stations_df['city'] == 'San Francisco']['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tribal-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(\"../../Results/Classification_patterns/Classifier_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 15\n",
    "window_width = 3\n",
    "split_date = \"2015-03-09 09:30:00\"\n",
    "time_zone = None\n",
    "\n",
    "tot = 0\n",
    "tot_na = 0\n",
    "\n",
    "for station in San_Fancisco_stations:\n",
    "    single_station_df = status_df[status_df['station_id']==station][['time', 'bikes_available', \"docks_available\"]]\n",
    "    single_station_df = single_station_df.resample(f\"{interval}T\", on = 'time').mean().reset_index()\n",
    "    count = len(single_station_df.loc[single_station_df['time'] >= split_date])\n",
    "    count_na = count - len(single_station_df.loc[single_station_df['time'] >= split_date].dropna())\n",
    "    tot += count\n",
    "    tot_na += count_na\n",
    "    print(count, count_na)\n",
    "print(\"Total values:\\t\\t\", tot)\n",
    "print(\"Total null values:\\t\", tot_na)\n",
    "print(\"Total no null:\\t\\t\", tot-tot_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "affiliated-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 15\n",
    "window_width = 3\n",
    "split_date = \"2015-03-09 09:30:00\"\n",
    "time_zone = '17-18'\n",
    "zone_str = f\"_{time_zone}\" if time_zone != None else \"\"\n",
    "\n",
    "if time_zone != None:\n",
    "        start_h, end_h = time_zone.split('-')\n",
    "        status_df = status_df[(status_df.time.dt.hour >= int(start_h)) & (status_df.time.dt.hour < int(end_h))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "instructional-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "for reference_station in San_Fancisco_stations:\n",
    "    merged_df = pd.DataFrame()\n",
    "    for current_station in San_Fancisco_stations:\n",
    "        new_df = pd.DataFrame()\n",
    "        single_station_df = status_df[status_df['station_id']==current_station][['time', 'bikes_available', \"docks_available\"]]\n",
    "        windowing_df =  single_station_df.resample(f\"{interval}T\", on = 'time').mean()\n",
    "        \n",
    "        #resetto l'indice\n",
    "        windowing_df  = windowing_df.reset_index()\n",
    "                 \n",
    "        new_df['time'] = windowing_df['time']\n",
    "        \n",
    "        #inserisco le colonne \n",
    "        for i in reversed(range(window_width)):\n",
    "            new_df[f'docks_av_{current_station}_T{i}'] =  windowing_df['docks_available'].shift(periods=i)\n",
    "            new_df[f'bikes_av_{current_station}_T{i}'] =  windowing_df['bikes_available'].shift(periods=i)\n",
    "        \n",
    "        #se la stazione corrente è quella di riferimento, inserisco la colonna \"status\", shiftata indietro di 1 \n",
    "        # slot temporale in modo che indichi lo stato denna stazione nello slot temporale successivo a quello di riferimento.\n",
    "        # Questa colonna fungerà da label\n",
    "        if (current_station == reference_station):\n",
    "            windowing_df[\"status\"] = np.where(windowing_df[\"docks_available\"] <= 2, \"QP\", \"N\")\n",
    "            new_df[\"status\"] =  windowing_df['status'].shift(periods=-1)\n",
    "            \n",
    "        if len(merged_df) == 0:\n",
    "            merged_df = new_df.copy(deep=True)\n",
    "            continue\n",
    "        merged_df = pd.merge(merged_df, new_df, how='outer', on='time')\n",
    "    \n",
    "    #Rimuovo tutti i record con valori mancanti\n",
    "    merged_df = merged_df.dropna()\n",
    "    \n",
    "    if not os.path.exists(f'../../Datasets/{interval}_{window_width}'):\n",
    "        os.makedirs(f\"../../Datasets/{interval}_{window_width}\")\n",
    "\n",
    "    #splitto in train e test set\n",
    "    train_df = merged_df.loc[merged_df['time'] < split_date].copy()\n",
    "    test_df = merged_df.loc[merged_df['time'] >= split_date].copy()\n",
    "    \n",
    "    #rimuovo il tempo in quanto non è utile ai fini della classificazione\n",
    "    train_df.drop(columns=[\"time\"], inplace=True) \n",
    "    test_df.drop(columns=[\"time\"], inplace=True) \n",
    "    \n",
    "    #salvo la versione senza l'informazione sul tempo\n",
    "    train_df.to_csv(f\"../../Datasets/{interval}_{window_width}/station{reference_station}{zone_str}_train.csv\", index=False)\n",
    "    test_df.to_csv(f\"../../Datasets/{interval}_{window_width}/station{reference_station}{zone_str}_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "living-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'DecisionTree': tree.DecisionTreeClassifier(random_state=42),   \n",
    "    'RandomForest': ensemble.RandomForestClassifier(random_state=42, max_features='sqrt'),\n",
    "    'XGBoost': xgb.XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "transsexual-branch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time of execution is: 1088.3197820186615 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for reference_station in San_Fancisco_stations:\n",
    "\n",
    "    train_df = pd.read_csv(f\"../../Datasets/{interval}_{window_width}/station{reference_station}{zone_str}_train.csv\")\n",
    "    \n",
    "    y = train_df['status']\n",
    "    X = train_df.drop(columns=['status'])\n",
    "    \n",
    "    #TRAINING\n",
    "    for classifier in classifiers.keys():\n",
    "        model = classifiers[classifier].fit(X, y)\n",
    "        \n",
    "        #salvo il modello\n",
    "        if not os.path.exists(f'../../Results/Other_classifiers/Trained_models/{classifier}/{interval}_{window_width}'):\n",
    "            os.makedirs(f\"../../Results/Other_classifiers/Trained_models/{classifier}/{interval}_{window_width}\")\n",
    "        dump(model, f'../../Results/Other_classifiers/Trained_models/{classifier}/{interval}_{window_width}/station{reference_station}{zone_str}_trained_model.joblib')\n",
    "\n",
    "end = time.time()\n",
    "print(f'The time of execution is: {end-start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_matrix = []\n",
    "\n",
    "if not os.path.exists('../../Results/Other_classifiers/Evaluation'):\n",
    "    os.makedirs('../../Results/Other_classifiers/Evaluation')\n",
    "\n",
    "for classifier in classifiers.keys():\n",
    "    \n",
    "    file = open(f'../../Results/Other_classifiers/Evaluation/{classifier}_{interval}_{window_width}{zone_str}.txt', \"w\")\n",
    "    file.write(f'TESING RESULTS FOR {classifier} CLASSIFIER:\\n\\n')\n",
    "\n",
    "    tot_fp = 0\n",
    "    tot_tp = 0\n",
    "    tot_fn = 0\n",
    "    tot_tn = 0\n",
    "    \n",
    "    for station_id in San_Fancisco_stations:\n",
    "        model = load(f'../../Results/Other_classifiers/Trained_models/{classifier}/{interval}_{window_width}/station{station_id}{zone_str}_trained_model.joblib')\n",
    "\n",
    "        test_df = pd.read_csv(f'../../Datasets/{interval}_{window_width}/station{station_id}{zone_str}_test.csv')\n",
    "        y_test = test_df['status']\n",
    "        X_test = test_df.drop(columns=['status'])    \n",
    "\n",
    "        prediction = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, prediction)\n",
    "\n",
    "        str_= f'{classifier} FOR STATION {station_id}' + '\\n'\n",
    "        str_ += f'Confusion matrix:' + '\\n'\n",
    "        str_ += str(cm) + '\\n'\n",
    "\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        str_+= f'tp={tp}, fn={fn}, fp={fp}, tn={tn}' +'\\n'\n",
    "\n",
    "        test_accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "        test_recall = (tp) / (tp + fn)\n",
    "        test_precision = (tp) / (tp + fp)\n",
    "        test_f1 = 2*test_precision*test_recall/(test_precision+test_recall)\n",
    "\n",
    "        str_+= f'accuracy={test_accuracy}; recall={test_recall}; precision={test_precision}; F1={test_f1}' +'\\n\\n'\n",
    "        str_+= \"-\"*10 +'\\n\\n'\n",
    "\n",
    "        tot_fp += fp\n",
    "        tot_tp += tp\n",
    "        tot_fn += fn\n",
    "        tot_tn += tn\n",
    "        \n",
    "        file.write(str_)\n",
    "\n",
    "    avg_accuracy = (tot_tn + tot_tp) / (tot_tn + tot_fp + tot_fn + tot_tp)\n",
    "    avg_recall = (tot_tp) / (tot_tp + tot_fn)\n",
    "    avg_precision = (tot_tp) / (tot_tp + tot_fp)\n",
    "    avg_f1 = 2*avg_precision*avg_recall/(avg_precision+avg_recall)\n",
    "    result_matrix.append([avg_accuracy, avg_recall, avg_precision, avg_f1])\n",
    "    \n",
    "    avg_str = f\"AVERAGE VALUES FOR {classifier}: accuracy={avg_accuracy}; recall={avg_recall}; precision={avg_precision}; F1={avg_f1}\"\n",
    "\n",
    "    file.write(avg_str)\n",
    "    file.close()\n",
    "    \n",
    "results_table = pd.DataFrame(result_matrix, columns=['avg_accuracy', 'avg_recall', 'avg_precision', 'avg_F1'], index=pd.Index(classifiers.keys()))\n",
    "results_table.to_csv(f\"../../Results/Other_classifiers/Evaluation/Overall_results_{interval}_{window_width}{zone_str}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
